{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7107b9-0c40-4a71-ae4e-8172521798e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939472bf-de0b-4957-8de2-799f2a3ba3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check for zero-only labels\n",
    "def is_zero_only(label_file):\n",
    "    label = nib.load(label_file).get_fdata()\n",
    "    return np.all(label == 0)\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, data_list, label_list, transform=None):\n",
    "        # Filter the data_list to include only the main files (those without additional suffixes)\n",
    "        self.data_list = self.filter_data_files(data_list)\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure the number of filtered data files matches the number of label files\n",
    "        assert len(self.data_list) == len(self.label_list), \"Mismatch between data files and labels\"\n",
    "\n",
    "    def filter_data_files(self, data_list):\n",
    "        # Filter data files to keep only those without the suffix \"_1\", \"_2\", etc.\n",
    "        filtered_data = [f for f in data_list if re.search(r\"_defected\\.nii\\.gz$\", f)]\n",
    "        return filtered_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_file = self.data_list[idx]\n",
    "        label_file = self.label_list[idx]\n",
    "\n",
    "        if not is_zero_only(label_file):\n",
    "            data = nib.load(data_file).get_fdata()\n",
    "            label = nib.load(label_file).get_fdata()\n",
    "\n",
    "            # Binarize the label (values become 0 or 1)\n",
    "            #label = np.where(label > 0, 1, 0).astype(np.float32)\n",
    "\n",
    "            # Convert to tensor (data shape: (128, 128, 64))\n",
    "            data_tensor = torch.from_numpy(data).float().unsqueeze(0)\n",
    "            label_tensor = torch.from_numpy(label).float().unsqueeze(0)\n",
    "\n",
    "            sample = {'data': data_tensor, 'label': label_tensor}\n",
    "\n",
    "            # Apply any transforms (e.g., resizing)\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "\n",
    "            return sample\n",
    "\n",
    "        # Raise error if the label is zero-only (shouldn't happen in practice)\n",
    "        raise IndexError(\"No valid samples available\")\n",
    "\n",
    "# Transform to resize the data\n",
    "class ResizeTransform:\n",
    "    def __init__(self, target_shape=(256, 256, 128)):\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample['data'], sample['label']\n",
    "        data = F.interpolate(data.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        label = F.interpolate(label.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        return {'data': data, 'label': label}\n",
    "\n",
    "# DataLoader creation\n",
    "def create_dataloader(data_list, label_list, transform=None, batch_size=2, shuffle=True, num_workers=32):\n",
    "    dataset = MedicalDataset(data_list, label_list, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "    return dataloader\n",
    "    \n",
    "# Define directories\n",
    "train_data_dir = '/workspace/RibCage/train-ribfrac-defected'\n",
    "train_label_dir = '/workspace/RibCage/train-segmented_ribfrac'\n",
    "val_data_dir = '/workspace/RibCage/val-ribfrac-defected'\n",
    "val_label_dir = '/workspace/RibCage/val-segmented_ribfrac'\n",
    "\n",
    "# Get list of files\n",
    "train_data_list = sorted(glob.glob(os.path.join(train_data_dir, '*.nii')) + glob.glob(os.path.join(train_data_dir, '*.nii.gz')))\n",
    "train_label_list = sorted(glob.glob(os.path.join(train_label_dir, '*.nii')) + glob.glob(os.path.join(train_label_dir, '*.nii.gz')))\n",
    "val_data_list = sorted(glob.glob(os.path.join(val_data_dir, '*.nii')) + glob.glob(os.path.join(val_data_dir, '*.nii.gz')))\n",
    "val_label_list = sorted(glob.glob(os.path.join(val_label_dir, '*.nii')) + glob.glob(os.path.join(val_label_dir, '*.nii.gz')))\n",
    "\n",
    "# Ensure data and labels are paired properly\n",
    "#assert len(train_data_list) == len(train_label_list), \"Training data and labels are not of the same length\"\n",
    "#assert len(val_data_list) == len(val_label_list), \"Validation data and labels are not of the same length\"\n",
    "\n",
    "# Define the transform\n",
    "resize_transform = ResizeTransform(target_shape=(256, 256, 128))\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = create_dataloader(train_data_list, train_label_list, transform=resize_transform, batch_size=2, shuffle=True)\n",
    "val_loader = create_dataloader(val_data_list, val_label_list, transform=resize_transform, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3016229a-5722-4555-bf5f-6b541dea6e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader for training data...\n",
      "Batch 1: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Batch 2: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Batch 3: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "\n",
      "Testing DataLoader for testing data...\n",
      "Batch 1: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Batch 2: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Batch 3: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "print(\"\\nTesting DataLoader for training data...\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    data_tensor = batch['data']\n",
    "    label_tensor = batch['label']\n",
    "    print(f'Batch {i + 1}: Data shape: {data_tensor.shape}, Label shape: {label_tensor.shape}')\n",
    "    if i == 2:  # Load a few batches for demonstration\n",
    "        break\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "print(\"\\nTesting DataLoader for testing data...\")\n",
    "for i, batch in enumerate(val_loader):\n",
    "    data_tensor = batch['data']\n",
    "    label_tensor = batch['label']\n",
    "    print(f'Batch {i + 1}: Data shape: {data_tensor.shape}, Label shape: {label_tensor.shape}')\n",
    "    if i == 2:  # Load a few batches for demonstration\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927a6899-a739-4f7b-8ee8-ca619943a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Importing Necessary Libraries...\n",
      "Necessary Libraries imported\n",
      "\n",
      "Data Loading Started...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Training data and labels are not of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m train_label_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_label_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.nii\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m+\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_label_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#est_data_list = sorted(glob.glob(os.path.join(test_data_dir, '*.nii')) + glob.glob(os.path.join(test_data_dir, '*.nii.gz')))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Ensure data and labels are paired properly\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_data_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_label_list), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data and labels are not of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Create DataLoader for training\u001b[39;00m\n\u001b[1;32m     86\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m create_dataloader(train_data_list, train_label_list, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Training data and labels are not of the same length"
     ]
    }
   ],
   "source": [
    "print(\"Started Importing Necessary Libraries...\")\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Necessary Libraries imported\")\n",
    "\n",
    "print(\"\\nData Loading Started...\")\n",
    "\n",
    "# Function to resize data\n",
    "def resizing(data, target_shape=(128, 128, 64)):\n",
    "    \"\"\"Resize the data to the target shape.\"\"\"\n",
    "    a, b, c = data.shape\n",
    "    return zoom(data, (target_shape[0] / a, target_shape[1] / b, target_shape[2] / c), order=2, mode='constant')\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, data_list, label_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list): List of paths to the data files.\n",
    "            label_list (list): List of paths to the label files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        assert len(data_list) == len(label_list), \"Data and label lists must be the same length\"\n",
    "        self.data_list = data_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_file = self.data_list[idx]\n",
    "        label_file = self.label_list[idx]\n",
    "\n",
    "        data = nib.load(data_file).get_fdata()\n",
    "        label = nib.load(label_file).get_fdata()\n",
    "\n",
    "        data_resized = resizing(data)\n",
    "        label_resized = resizing(label)\n",
    "\n",
    "        #data_resized = np.expand_dims(data_resized, axis=(0, 1))\n",
    "        #label_resized = np.expand_dims(label_resized, axis=(0, 1))\n",
    "\n",
    "        data_tensor = torch.from_numpy(data_resized).float()\n",
    "        label_tensor = torch.from_numpy(label_resized).float()\n",
    "\n",
    "        sample = {'data': data_tensor, 'label': label_tensor}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Create DataLoader\n",
    "def create_dataloader(data_list, label_list, batch_size=4, shuffle=True, num_workers=2):\n",
    "    \"\"\"Create DataLoader for the dataset.\"\"\"\n",
    "    dataset = MedicalDataset(data_list, label_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Define directories\n",
    "train_data_dir = '/workspace/RibCage/train-ribfrac-defected'\n",
    "train_label_dir = '/workspace/RibCage/train-segmented_ribfrac'\n",
    "\n",
    "# Get list of files\n",
    "train_data_list = sorted(glob.glob(os.path.join(train_data_dir, '*.nii')) + glob.glob(os.path.join(train_data_dir, '*.nii.gz')))\n",
    "train_label_list = sorted(glob.glob(os.path.join(train_label_dir, '*.nii')) + glob.glob(os.path.join(train_label_dir, '*.nii.gz')))\n",
    "#est_data_list = sorted(glob.glob(os.path.join(test_data_dir, '*.nii')) + glob.glob(os.path.join(test_data_dir, '*.nii.gz')))\n",
    "\n",
    "# Ensure data and labels are paired properly\n",
    "assert len(train_data_list) == len(train_label_list), \"Training data and labels are not of the same length\"\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = create_dataloader(train_data_list, train_label_list, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Train Loader has been created...\")\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "print(\"\\nTesting DataLoader for training data...\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    data_tensor = batch['data']\n",
    "    label_tensor = batch['label']\n",
    "    print(f'Batch {i + 1}: Data shape: {data_tensor.shape}, Label shape: {label_tensor.shape}')\n",
    "    if i == 2:  # Load a few batches for demonstration\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471084e4-46b2-4097-a61a-4cbcbc67d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved resized data sample to /workspace/RibCage/resized_samples/resized_data_sample2.nii.gz\n",
      "Saved resized label sample to /workspace/RibCage/resized_samples/resized_label_sample2.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "def save_sample_from_loader(dataloader, output_dir, sample_index=0):\n",
    "    \"\"\" Save a sample from the DataLoader to a file after resizing. \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the DataLoader to get a batch\n",
    "    for batch in dataloader:\n",
    "        # Extract the sample (assuming sample_index < batch size)\n",
    "        data_img = batch['data'][sample_index].cpu().numpy()\n",
    "        label_img = batch['label'][sample_index].cpu().numpy()\n",
    "\n",
    "        # Create NIfTI images\n",
    "        data_img_nifti = nib.Nifti1Image(data_img.squeeze(), np.eye(4))\n",
    "        label_img_nifti = nib.Nifti1Image(label_img.squeeze(), np.eye(4))\n",
    "\n",
    "        # Define file paths\n",
    "        data_file_path = os.path.join(output_dir, 'resized_data_sample2.nii.gz')\n",
    "        label_file_path = os.path.join(output_dir, 'resized_label_sample2.nii.gz')\n",
    "\n",
    "        # Save the images\n",
    "        nib.save(data_img_nifti, data_file_path)\n",
    "        nib.save(label_img_nifti, label_file_path)\n",
    "        \n",
    "        print(f\"Saved resized data sample to {data_file_path}\")\n",
    "        print(f\"Saved resized label sample to {label_file_path}\")\n",
    "        \n",
    "        break  # Only save one sample\n",
    "\n",
    "# Define output directory\n",
    "output_dir = '/workspace/RibCage/resized_samples'\n",
    "\n",
    "# Save a sample from the training DataLoader\n",
    "save_sample_from_loader(train_loader, output_dir, sample_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabe9fec-0b39-44c6-9fc9-5ce958bdac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5754d17229074fa5bc6a1c7139843e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=162, description='slice_index', max=324), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Load the NIfTI file\n",
    "nifti_file = '/workspace/RibCage/val-ribfrac-implants/RibFrac421_implant.nii.gz'\n",
    "nifti_image = nib.load(nifti_file)\n",
    "ct_volume = nifti_image.get_fdata()  # Get the image data as a NumPy array\n",
    "\n",
    "# Function to display a single axial slice\n",
    "def show_axial_slice(slice_index):\n",
    "    plt.imshow(ct_volume[:, :, slice_index], cmap='gray')\n",
    "    plt.title(f'Axial Slice {slice_index}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive slider to browse through axial slices\n",
    "interact(show_axial_slice, slice_index=(0, ct_volume.shape[2] - 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ca2d6-db04-4870-9d5d-4729926ef165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
