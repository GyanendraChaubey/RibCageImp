{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9996b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import segmentation_models_pytorch_3d as smp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc8ac5f-d126-4c19-baad-816d8720aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice coefficient for binary segmentation\n",
    "def dice_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the Dice coefficient between the prediction and the ground truth.\n",
    "    Handles cases where the output or ground truth is all zeros.\n",
    "    \"\"\"\n",
    "    # Binarize the predictions (threshold at 0.0)\n",
    "    pred = (pred > 0.0).float()  # Convert probabilities/logits to binary predictions\n",
    "    target = (target > 0.0).float()  # Ensure target is binary\n",
    "\n",
    "    # Flatten the arrays to compare voxel-wise\n",
    "    pred_flat = pred.view(-1)  # Flatten prediction\n",
    "    target_flat = target.view(-1)  # Flatten target\n",
    "\n",
    "    # Calculate intersection and Dice score\n",
    "    intersection = (pred_flat * target_flat).sum()  # True positive (prediction == 1 and target == 1)\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2217de-3724-491b-9ad2-4e1049211359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hausdorff_distance(pred, target):\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    target_np = target.cpu().numpy()\n",
    "    pred_points = np.argwhere(pred_np > 0)\n",
    "    target_points = np.argwhere(target_np > 0)\n",
    "    return max(directed_hausdorff(pred_points, target_points)[0], directed_hausdorff(target_points, pred_points)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281c993a-8aa4-48b7-96b4-5a24d72defb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIfTI saving function\n",
    "def save_nifti(data_tensor, label_tensor, save_dir, batch_idx):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    data_np = data_tensor.cpu().numpy().astype(np.float32).squeeze(1)\n",
    "    label_np = label_tensor.cpu().numpy().astype(np.float32).squeeze(1)\n",
    "\n",
    "    for i in range(data_np.shape[0]):\n",
    "        data_filename = os.path.join(save_dir, f'test_data_batch{batch_idx}_instance_{i}.nii.gz')\n",
    "        label_filename = os.path.join(save_dir, f'test_label_batch{batch_idx}_instance_{i}.nii.gz')\n",
    "\n",
    "        nib.save(nib.Nifti1Image(data_np[i], np.eye(4)), data_filename)\n",
    "        nib.save(nib.Nifti1Image(label_np[i], np.eye(4)), label_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3eda007-df18-4319-a590-97523bb0bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ribfrac_number(filename):\n",
    "    base = os.path.basename(filename)\n",
    "    match = re.search(r'RibFrac(\\d+)', base)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def find_matching_files(data_files, label_files):\n",
    "    data_dict = {extract_ribfrac_number(f): f for f in data_files}\n",
    "    label_dict = {extract_ribfrac_number(f): f for f in label_files}\n",
    "    \n",
    "    matched_pairs = []\n",
    "    for num in data_dict.keys():\n",
    "        if num in label_dict:\n",
    "            matched_pairs.append((data_dict[num], label_dict[num]))\n",
    "        elif num - 1 in label_dict:  # Check for off-by-one match\n",
    "            matched_pairs.append((data_dict[num], label_dict[num - 1]))\n",
    "    \n",
    "    return matched_pairs\n",
    "\n",
    "def is_valid_pair(data_file, label_file):\n",
    "    #logger.debug(f\"Checking pair: {os.path.basename(data_file)} - {os.path.basename(label_file)}\")\n",
    "    \n",
    "    try:\n",
    "        label = nib.load(label_file).get_fdata()\n",
    "        if np.all(label == 0) or np.isnan(label).any() or np.isinf(label).any():\n",
    "            #logger.debug(f\"Label file contains invalid data (all zeros, NaNs, or Infs): {label_file}\")\n",
    "            return False\n",
    "        #logger.debug(f\"Valid pair: {os.path.basename(data_file)} - {os.path.basename(label_file)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        #logger.error(f\"Error loading {label_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_for_invalid_values(tensor, tensor_name=\"tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {tensor_name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"Inf detected in {tensor_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18a7a68-30dd-4342-92a2-a0dfe004007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class with Dynamic Filtering\n",
    "class MedicalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_list, label_list, transform=None):\n",
    "        #logger.info(f\"Initializing dataset with {len(data_list)} data files and {len(label_list)} label files\")\n",
    "        \n",
    "        self.matched_pairs = find_matching_files(data_list, label_list)\n",
    "        self.valid_pairs = [pair for pair in self.matched_pairs if is_valid_pair(*pair)]\n",
    "        \n",
    "        #logger.info(f\"Total pairs: {len(self.matched_pairs)}, Valid pairs: {len(self.valid_pairs)}\")\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        # Log all valid pairs\n",
    "        #for data, label in self.valid_pairs:\n",
    "         #   logger.debug(f\"Valid pair: {os.path.basename(data)} - {os.path.basename(label)}\")\n",
    "\n",
    "        #self.mean = mean\n",
    "        #self.std = std\n",
    "\n",
    "    #def normalize(self, tensor, mean, std):\n",
    "    #    return (tensor - mean) / std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.valid_pairs):\n",
    "            raise IndexError(f\"Index {idx} out of range for valid_pairs.\")\n",
    "        \n",
    "        data_file, label_file = self.valid_pairs[idx]\n",
    "\n",
    "        # Load the data and label\n",
    "        data = nib.load(data_file).get_fdata()\n",
    "        label = nib.load(label_file).get_fdata()\n",
    "\n",
    "        #logger.debug(f\"File: {os.path.basename(data_file)} - Raw data shape: {data.shape}\")\n",
    "        #logger.debug(f\"File: {os.path.basename(label_file)} - Raw label shape: {label.shape}\")\n",
    "\n",
    "        # Convert data and label to tensors\n",
    "        data_tensor = torch.from_numpy(data).float().unsqueeze(0)\n",
    "        label_tensor = torch.from_numpy(label).float().unsqueeze(0)\n",
    "\n",
    "        # Check for invalid values in data and label\n",
    "        check_for_invalid_values(data_tensor, \"data_tensor\")\n",
    "        check_for_invalid_values(label_tensor, \"label_tensor\")\n",
    "\n",
    "        # Normalize tensors\n",
    "        #data_tensor = self.normalize(data_tensor, self.mean, self.std)\n",
    "        #label_tensor = self.normalize(label_tensor, self.mean, self.std)\n",
    "\n",
    "        # Log stats to check ranges\n",
    "        #logger.debug(f\"Data tensor min: {data_tensor.min()}, max: {data_tensor.max()}, mean: {data_tensor.mean()}\")\n",
    "        #logger.debug(f\"Label tensor min: {label_tensor.min()}, max: {label_tensor.max()}, mean: {label_tensor.mean()}\")\n",
    "\n",
    "        sample = {'data': data_tensor, 'label': label_tensor, 'data_file': data_file, 'label_file': label_file}\n",
    "\n",
    "        # Apply any transforms (e.g., resizing)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Transform to resize the data\n",
    "class ResizeTransform:\n",
    "    def __init__(self, target_shape=(256, 256, 128)):\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample['data'], sample['label']\n",
    "        data = F.interpolate(data.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        #label = F.interpolate(label.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        label = F.interpolate(label.unsqueeze(0), size=self.target_shape, mode='nearest').squeeze(0)\n",
    "        #logger.debug(f\"Transform data shape: {data.shape}\")\n",
    "        #logger.debug(f\"Transform label shape: {label.shape}\")\n",
    "        return {'data': data, 'label': label, 'data_file': sample['data_file'], 'label_file': sample['label_file']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f1f7f7-549d-476f-ae88-d90f9df69ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader creation function\n",
    "def create_dataloader(data_list, label_list, transform=None, batch_size=2, shuffle=True, num_workers=8):\n",
    "    dataset = MedicalDataset(data_list, label_list, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "test_data_dir = '/workspace/RibCage/test-ribfrac-defected'  \n",
    "test_label_dir = '/workspace/RibCage/test-ribfrac-implants' \n",
    "val_data_dir = '/workspace/RibCage/val-ribfrac-defected'\n",
    "val_label_dir = '/workspace/RibCage/val-ribfrac-implants'\n",
    "\n",
    "test_data_list = sorted(glob.glob(os.path.join(test_data_dir, '*.nii')) + glob.glob(os.path.join(test_data_dir, '*.nii.gz')))\n",
    "test_label_list = sorted(glob.glob(os.path.join(test_label_dir, '*.nii')) + glob.glob(os.path.join(test_label_dir, '*.nii.gz')))\n",
    "val_data_list = sorted(glob.glob(os.path.join(val_data_dir, '*.nii')) + glob.glob(os.path.join(val_data_dir, '*.nii.gz')))\n",
    "val_label_list = sorted(glob.glob(os.path.join(val_label_dir, '*.nii')) + glob.glob(os.path.join(val_label_dir, '*.nii.gz')))\n",
    "\n",
    "# Transform (resize if necessary)\n",
    "resize_transform = ResizeTransform(target_shape=(256, 256, 128))\n",
    "\n",
    "temp_test_dataset = MedicalDataset(test_data_list, test_label_list, transform=resize_transform) \n",
    "#test_mean, test_std = compute_mean_std(temp_test_dataset)\n",
    "\n",
    "temp_val_dataset = MedicalDataset(val_data_list, val_label_list, transform=resize_transform) \n",
    "#val_mean, val_std = compute_mean_std(temp_val_dataset)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = create_dataloader(test_data_list, test_label_list,   transform=resize_transform, batch_size=2, shuffle=False)\n",
    "val_loader = create_dataloader(val_data_list, val_label_list, transform=resize_transform, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051e3b6b-f5bd-42ea-ba68-4f2cfa02a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and metric calculation function\n",
    "def inference_and_evaluate_dice_model(model, test_loader, device, save_dir):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    dice_scores = []\n",
    "    hausdorff_distances = []\n",
    "    mse_scores = []\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            inputs = batch['data'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            #print(inputs.shape)\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            labels = labels.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "            # Forward pass\n",
    "            #features = model.encoder(inputs)\n",
    "            #decoder_output = model.decoder(*features)\n",
    "            #outputs = model.segmentation_head(decoder_output)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #print(f\"Val output shape: {outputs.shape}\")\n",
    "            #print(f\"Val labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Apply sigmoid to get probabilities\n",
    "            #outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Save output predictions\n",
    "            #save_nifti(outputs, labels, save_dir, batch_idx)\n",
    "            \n",
    "            # Compute Dice Score\n",
    "            dice = dice_score(outputs, labels)\n",
    "            dice_scores.append(dice)\n",
    "\n",
    "            # Compute Hausdorff Distance\n",
    "            hausdorff = hausdorff_distance(outputs, labels)\n",
    "            hausdorff_distances.append(hausdorff)\n",
    "\n",
    "            # Compute MSE\n",
    "            mse = mean_squared_error(labels.cpu().numpy().ravel(), outputs.cpu().numpy().ravel())\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "            if dice > 0.30:\n",
    "                dice_scores.append(dice)\n",
    "                hausdorff_distances.append(hausdorff)\n",
    "                mse_scores.append(mse)\n",
    "\n",
    "            print(f'Batch {batch_idx} - Dice: {dice:.4f}, Hausdorff: {hausdorff:.4f}, MSE: {mse:.4f}')\n",
    "    \n",
    "    # Save overall metrics\n",
    "    #np.save(os.path.join(save_dir, 'dice_scores.npy'), np.array(dice_scores))\n",
    "    #np.save(os.path.join(save_dir, 'hausdorff_distances.npy'), np.array(hausdorff_distances))\n",
    "    #np.save(os.path.join(save_dir, 'mse_scores.npy'), np.array(mse_scores))\n",
    "\n",
    "    print(f'Average Dice Score: {np.mean(dice_scores):.4f}, len: {len(dice_scores)}')\n",
    "    print(f'Average Hausdorff Distance: {np.mean(hausdorff_distances):.4f}')\n",
    "    print(f'Average MSE: {np.mean(mse_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a74f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutional layer with initialization\n",
    "class Conv3dLayer(nn.Module):\n",
    "    def __init__(self, input_chn, output_chn, kernel_size, stride, bias=False):\n",
    "        super(Conv3dLayer, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2  # Calculate padding\n",
    "        self.conv = nn.Conv3d(input_chn, output_chn, kernel_size, stride, padding=padding, bias=use_bias)\n",
    "        nn.init.trunc_normal_(self.conv.weight, std=0.01)\n",
    "        if use_bias:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Define the block with convolution, batch normalization, and ReLU\n",
    "class ConvBnReLU(nn.Module):\n",
    "    def __init__(self, input_chn, output_chn, kernel_size, stride):\n",
    "        super(ConvBnReLU, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv3d(input_chn, output_chn, kernel_size, stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm3d(output_chn, momentum=0.9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the deconvolutional layer with initialization\n",
    "class Deconv3dLayer(nn.Module):\n",
    "    def __init__(self, input_chn, output_chn):\n",
    "        super(Deconv3dLayer, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose3d(input_chn, output_chn, kernel_size=4, stride=2, padding=1)\n",
    "        nn.init.normal_(self.deconv.weight, std=0.01)\n",
    "        nn.init.zeros_(self.deconv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.deconv(x)\n",
    "\n",
    "# Define the block with deconvolution, batch normalization, and ReLU\n",
    "class DeconvBnReLU(nn.Module):\n",
    "    def __init__(self, input_chn, output_chn):\n",
    "        super(DeconvBnReLU, self).__init__()\n",
    "        self.deconv = Deconv3dLayer(input_chn, output_chn)\n",
    "        self.bn = nn.BatchNorm3d(output_chn, momentum=0.9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the block with repeated convolution, batch normalization, and ReLU\n",
    "class ConvBnReLUX3(nn.Module):\n",
    "    def __init__(self, input_chn, output_chn, kernel_size, stride, use_bias):\n",
    "        super(ConvBnReLUX3, self).__init__()\n",
    "        self.conv1 = ConvBnReLU(input_chn, output_chn, kernel_size, stride, use_bias)\n",
    "        self.conv2 = ConvBnReLU(output_chn, output_chn, kernel_size, stride, use_bias)\n",
    "        self.conv3 = ConvBnReLU(output_chn, output_chn, kernel_size, stride, use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv1(x)\n",
    "        z_out = self.conv2(z)\n",
    "        z_out = self.conv3(z_out)\n",
    "        return z + z_out        \n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.conv1 = ConvBnReLU(2, 64, kernel_size=5, stride=2)\n",
    "        self.conv2 = ConvBnReLU(64, 128, kernel_size=5, stride=2)\n",
    "        self.conv3 = ConvBnReLU(128, 256, kernel_size=5, stride=2)\n",
    "        self.conv4 = ConvBnReLU(256, 512, kernel_size=5, stride=2)\n",
    "        self.conv5 = ConvBnReLU(512, 512, kernel_size=5, stride=1)\n",
    "        self.deconv1 = DeconvBnReLU(512, 256)\n",
    "        self.deconv2 = DeconvBnReLU(256, 128)\n",
    "        self.deconv3 = DeconvBnReLU(128, 64)\n",
    "        self.deconv4 = DeconvBnReLU(64, 32)\n",
    "        self.pred_prob1 = ConvBnReLU(32, 2, kernel_size=5, stride=1)\n",
    "        self.pred_prob2 = nn.Conv3d(2, 2, kernel_size=5, stride=1, padding='same', bias=True)\n",
    "        self.pred_prob3 = nn.Conv3d(2, 2, kernel_size=5, stride=1, padding='same', bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"input x:\",x.shape)\n",
    "        conv1 = self.conv1(x)\n",
    "        #print(\"input conv1:\",conv1.shape)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        #print(\"input conv2:\",conv2.shape)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        #print(\"input conv3:\",conv3.shape)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        #print(\"input conv4:\",conv4.shape)\n",
    "        conv5 = self.conv5(conv4)\n",
    "        #print(\"input conv5:\",conv5.shape)\n",
    "        deconv1 = self.deconv1(conv5)\n",
    "        #print(\"input deconv1:\",deconv1.shape)\n",
    "        deconv2 = self.deconv2(deconv1)\n",
    "        #print(\"input deconv2:\",deconv2.shape)\n",
    "        deconv3 = self.deconv3(deconv2)\n",
    "        #print(\"input deconv3:\",deconv3.shape)\n",
    "        deconv4 = self.deconv4(deconv3)\n",
    "        #print(\"input deconv4:\",deconv4.shape)\n",
    "        pred_prob1 = self.pred_prob1(deconv4)\n",
    "        pred_prob2 = self.pred_prob2(pred_prob1)\n",
    "        pred_prob3 = self.pred_prob3(pred_prob2)\n",
    "        #soft_prob = self.softmax(pred_prob3)\n",
    "        return pred_prob3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64b9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ac9d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad6f7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24ea816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('/workspace/RibCage/RibCageImplant/src/checkpoint_epoch_100.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7248381-20ce-436f-beea-52989f35b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - Dice: 0.0006, Hausdorff: 198.0227, MSE: 365856.2188\n",
      "Batch 1 - Dice: 0.0004, Hausdorff: 264.0417, MSE: 395059.1875\n",
      "Batch 2 - Dice: 0.0015, Hausdorff: 266.5914, MSE: 371341.9375\n",
      "Batch 3 - Dice: 0.0000, Hausdorff: 261.0843, MSE: 343632.7812\n",
      "Batch 4 - Dice: 0.0006, Hausdorff: 211.6743, MSE: 373413.0000\n",
      "Batch 5 - Dice: 0.0006, Hausdorff: 180.0278, MSE: 402193.8125\n",
      "Batch 6 - Dice: 0.0000, Hausdorff: 244.4974, MSE: 379551.5938\n",
      "Batch 7 - Dice: 0.0000, Hausdorff: 251.8432, MSE: 328940.1562\n",
      "Batch 8 - Dice: 0.0011, Hausdorff: 226.8127, MSE: 362976.2812\n",
      "Batch 9 - Dice: 0.0002, Hausdorff: 197.0330, MSE: 381851.5000\n",
      "Batch 10 - Dice: 0.0001, Hausdorff: 235.9428, MSE: 289615.6250\n",
      "Batch 11 - Dice: 0.0004, Hausdorff: 279.1863, MSE: 445965.8125\n",
      "Batch 12 - Dice: 0.0006, Hausdorff: 199.2963, MSE: 390492.1250\n",
      "Batch 13 - Dice: 0.0006, Hausdorff: 181.3422, MSE: 333328.9375\n",
      "Batch 14 - Dice: 0.0002, Hausdorff: 237.0253, MSE: 380598.9688\n",
      "Batch 15 - Dice: 0.0003, Hausdorff: 197.9217, MSE: 318811.1875\n",
      "Batch 16 - Dice: 0.0005, Hausdorff: 265.4016, MSE: 378132.0625\n",
      "Batch 17 - Dice: 0.0006, Hausdorff: 207.7017, MSE: 429589.3125\n",
      "Batch 18 - Dice: 0.0010, Hausdorff: 226.8854, MSE: 387800.4375\n",
      "Batch 19 - Dice: 0.0005, Hausdorff: 223.1703, MSE: 364848.3750\n",
      "Batch 20 - Dice: 0.0018, Hausdorff: 223.7700, MSE: 395092.1250\n",
      "Batch 21 - Dice: 0.0055, Hausdorff: 201.0423, MSE: 427955.0312\n",
      "Batch 22 - Dice: 0.0019, Hausdorff: 212.0542, MSE: 404445.3438\n",
      "Batch 23 - Dice: 0.0008, Hausdorff: 261.9046, MSE: 441848.0625\n",
      "Batch 24 - Dice: 0.0004, Hausdorff: 211.5230, MSE: 399613.0312\n",
      "Batch 25 - Dice: 0.0002, Hausdorff: 188.6240, MSE: 357916.0312\n",
      "Batch 26 - Dice: 0.0001, Hausdorff: 204.6998, MSE: 376839.1250\n",
      "Batch 27 - Dice: 0.0003, Hausdorff: 218.5269, MSE: 330987.4688\n",
      "Batch 28 - Dice: 0.0003, Hausdorff: 300.2366, MSE: 444700.7812\n",
      "Batch 29 - Dice: 0.0002, Hausdorff: 204.1813, MSE: 377348.5312\n",
      "Batch 30 - Dice: 0.0009, Hausdorff: 221.5875, MSE: 401229.5000\n",
      "Batch 31 - Dice: 0.0029, Hausdorff: 250.0160, MSE: 270637.2812\n",
      "Batch 32 - Dice: 0.0008, Hausdorff: 266.2386, MSE: 368836.7500\n",
      "Batch 33 - Dice: 0.0026, Hausdorff: 170.3673, MSE: 338719.4375\n",
      "Average Dice Score: 0.0008, len: 34\n",
      "Average Hausdorff Distance: 226.1845\n",
      "Average MSE: 375299.0312\n"
     ]
    }
   ],
   "source": [
    "# Directory to save inference results\n",
    "inference_save_dir = '/workspace/RibCage/saved_test_results_dice1/'\n",
    "\n",
    "# Perform inference on the test dataset and save results\n",
    "inference_and_evaluate_dice_model(model, val_loader, device, inference_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076c162-50e2-483f-b97d-f83796722a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
