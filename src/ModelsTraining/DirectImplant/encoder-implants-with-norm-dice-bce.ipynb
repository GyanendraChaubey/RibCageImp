{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aabe6a6-0ce7-4424-9ac7-94340584b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import segmentation_models_pytorch_3d as smp\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52239e7b-3bf0-486b-931e-bace21a3d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "log_filename = f'implants-ribcage-encoder_dice_bce_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=log_filename, encoding='utf-8', level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger.info(\"Started Importing Necessary Libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89efa277-4d48-4b01-8125-85fb3299fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ribfrac_number(filename):\n",
    "    base = os.path.basename(filename)\n",
    "    match = re.search(r'RibFrac(\\d+)', base)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def find_matching_files(data_files, label_files):\n",
    "    data_dict = {extract_ribfrac_number(f): f for f in data_files}\n",
    "    label_dict = {extract_ribfrac_number(f): f for f in label_files}\n",
    "    \n",
    "    matched_pairs = []\n",
    "    for num in data_dict.keys():\n",
    "        if num in label_dict:\n",
    "            matched_pairs.append((data_dict[num], label_dict[num]))\n",
    "        elif num - 1 in label_dict:  # Check for off-by-one match\n",
    "            matched_pairs.append((data_dict[num], label_dict[num - 1]))\n",
    "    \n",
    "    return matched_pairs\n",
    "\n",
    "def is_valid_pair(data_file, label_file):\n",
    "    #logger.debug(f\"Checking pair: {os.path.basename(data_file)} - {os.path.basename(label_file)}\")\n",
    "    \n",
    "    try:\n",
    "        label = nib.load(label_file).get_fdata()\n",
    "        if np.all(label == 0) or np.isnan(label).any() or np.isinf(label).any():\n",
    "            #logger.debug(f\"Label file contains invalid data (all zeros, NaNs, or Infs): {label_file}\")\n",
    "            return False\n",
    "        #logger.debug(f\"Valid pair: {os.path.basename(data_file)} - {os.path.basename(label_file)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        #logger.error(f\"Error loading {label_file}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_for_invalid_values(tensor, tensor_name=\"tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        logger.error(f\"NaN detected in {tensor_name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        logger.error(f\"Inf detected in {tensor_name}\")\n",
    "\n",
    "def compute_mean_std(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=10, num_workers=0, shuffle=False)\n",
    "    \n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        data = batch['data']\n",
    "        batch_samples = data.size(0)  # Get the batch size\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # Flatten the data\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1813d1-6d29-413f-9751-d6897190d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class with Dynamic Filtering\n",
    "class MedicalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_list, label_list, mean, std, transform=None):\n",
    "        logger.info(f\"Initializing dataset with {len(data_list)} data files and {len(label_list)} label files\")\n",
    "        \n",
    "        self.matched_pairs = find_matching_files(data_list, label_list)\n",
    "        self.valid_pairs = [pair for pair in self.matched_pairs if is_valid_pair(*pair)]\n",
    "        \n",
    "        #logger.info(f\"Total pairs: {len(self.matched_pairs)}, Valid pairs: {len(self.valid_pairs)}\")\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        # Log all valid pairs\n",
    "        #for data, label in self.valid_pairs:\n",
    "         #   logger.debug(f\"Valid pair: {os.path.basename(data)} - {os.path.basename(label)}\")\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def normalize(self, tensor, mean, std):\n",
    "        return (tensor - mean) / std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.valid_pairs):\n",
    "            raise IndexError(f\"Index {idx} out of range for valid_pairs.\")\n",
    "        \n",
    "        data_file, label_file = self.valid_pairs[idx]\n",
    "\n",
    "        # Load the data and label\n",
    "        data = nib.load(data_file).get_fdata()\n",
    "        label = nib.load(label_file).get_fdata()\n",
    "\n",
    "        #logger.debug(f\"File: {os.path.basename(data_file)} - Raw data shape: {data.shape}\")\n",
    "        #logger.debug(f\"File: {os.path.basename(label_file)} - Raw label shape: {label.shape}\")\n",
    "\n",
    "        # Convert data and label to tensors\n",
    "        data_tensor = torch.from_numpy(data).float().unsqueeze(0)\n",
    "        label_tensor = torch.from_numpy(label).float().unsqueeze(0)\n",
    "\n",
    "        # Check for invalid values in data and label\n",
    "        check_for_invalid_values(data_tensor, \"data_tensor\")\n",
    "        check_for_invalid_values(label_tensor, \"label_tensor\")\n",
    "\n",
    "        # Normalize tensors\n",
    "        data_tensor = self.normalize(data_tensor, self.mean, self.std)\n",
    "        label_tensor = self.normalize(label_tensor, self.mean, self.std)\n",
    "\n",
    "        # Log stats to check ranges\n",
    "        #logger.debug(f\"Data tensor min: {data_tensor.min()}, max: {data_tensor.max()}, mean: {data_tensor.mean()}\")\n",
    "        #logger.debug(f\"Label tensor min: {label_tensor.min()}, max: {label_tensor.max()}, mean: {label_tensor.mean()}\")\n",
    "\n",
    "        sample = {'data': data_tensor, 'label': label_tensor, 'data_file': data_file, 'label_file': label_file}\n",
    "\n",
    "        # Apply any transforms (e.g., resizing)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Transform to resize the data\n",
    "class ResizeTransform:\n",
    "    def __init__(self, target_shape=(256, 256, 128)):\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample['data'], sample['label']\n",
    "        data = F.interpolate(data.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        #label = F.interpolate(label.unsqueeze(0), size=self.target_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        label = F.interpolate(label.unsqueeze(0), size=self.target_shape, mode='nearest').squeeze(0)\n",
    "        #logger.debug(f\"Transform data shape: {data.shape}\")\n",
    "        #logger.debug(f\"Transform label shape: {label.shape}\")\n",
    "        return {'data': data, 'label': label, 'data_file': sample['data_file'], 'label_file': sample['label_file']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf895c23-bfbc-4041-a04b-2f38769399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader creation function\n",
    "def create_dataloader(data_list, label_list, mean, std, transform=None, batch_size=2, shuffle=True, num_workers=8):\n",
    "    dataset = MedicalDataset(data_list, label_list, mean, std, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# Define directories\n",
    "train_data_dir = '/workspace/RibCage/train-ribfrac-defected'  # defected rib\n",
    "train_label_dir = '/workspace/RibCage/train-ribfrac-implants'  # original rib\n",
    "val_data_dir = '/workspace/RibCage/val-ribfrac-defected'\n",
    "val_label_dir = '/workspace/RibCage/val-ribfrac-implants'\n",
    "\n",
    "# Get list of files\n",
    "train_data_list = sorted(glob.glob(os.path.join(train_data_dir, '*.nii')) + glob.glob(os.path.join(train_data_dir, '*.nii.gz')))\n",
    "train_label_list = sorted(glob.glob(os.path.join(train_label_dir, '*.nii')) + glob.glob(os.path.join(train_label_dir, '*.nii.gz')))\n",
    "val_data_list = sorted(glob.glob(os.path.join(val_data_dir, '*.nii')) + glob.glob(os.path.join(val_data_dir, '*.nii.gz')))\n",
    "val_label_list = sorted(glob.glob(os.path.join(val_label_dir, '*.nii')) + glob.glob(os.path.join(val_label_dir, '*.nii.gz')))\n",
    "\n",
    "# Define the transform\n",
    "resize_transform = ResizeTransform(target_shape=(256, 256, 128))\n",
    "\n",
    "temp_train_dataset = MedicalDataset(train_data_list, train_label_list, mean=0, std=1, transform=resize_transform) \n",
    "train_mean, train_std = compute_mean_std(temp_train_dataset)\n",
    "\n",
    "temp_val_dataset = MedicalDataset(val_data_list, val_label_list, mean=0, std=1, transform=resize_transform) \n",
    "val_mean, val_std = compute_mean_std(temp_val_dataset)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = create_dataloader(train_data_list, train_label_list,train_mean,train_std, transform=resize_transform, batch_size=2, shuffle=True)\n",
    "val_loader = create_dataloader(val_data_list, val_label_list,val_mean,val_std, transform=resize_transform, batch_size=2, shuffle=False)\n",
    "\n",
    "# Log the number of batches in each loader\n",
    "logger.info(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "logger.info(f\"Number of batches in val_loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c43243-8315-4b6e-bc45-e6d9b479461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and log a few samples from a dataloader\n",
    "def check_dataloader(loader, name):\n",
    "    logger.info(f\"Checking {name} dataloader:\")\n",
    "    for i, batch in enumerate(loader):\n",
    "        logger.info(f\"Batch {i}:\")\n",
    "        for j in range(len(batch['data'])):\n",
    "            logger.info(f\"  Sample {j}:\")\n",
    "            logger.info(f\"    Data file: {os.path.basename(batch['data_file'][j])}\")\n",
    "            logger.info(f\"    Label file: {os.path.basename(batch['label_file'][j])}\")\n",
    "        if i == 2:  # Check only first 3 batches\n",
    "            break\n",
    "\n",
    "# Check both dataloaders\n",
    "check_dataloader(train_loader, \"Training\")\n",
    "check_dataloader(val_loader, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7d32c2-1c6a-4195-a5f2-28ffdf7d4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data and label as NIfTI files\n",
    "def save_nifti(data_tensor, label_tensor, save_dir, batch_idx, is_train=True):\n",
    "    mode = 'train' if is_train else 'val'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert tensors to numpy arrays and detach from GPU (if applicable)\n",
    "    data_np = data_tensor.cpu().numpy().astype(np.float32)  # Convert to NumPy and ensure float32\n",
    "    label_np = label_tensor.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    logger.debug(f\"Original data shape: {data_np.shape}\")\n",
    "    logger.debug(f\"Original label shape: {label_np.shape}\")\n",
    "    \n",
    "    # Remove the singleton dimension (the 1 in the second position)\n",
    "    data_np = data_np.squeeze(1)\n",
    "    label_np = label_np.squeeze(1)\n",
    "\n",
    "    logger.debug(f\"Squeezed data shape: {data_np.shape}\")\n",
    "    logger.debug(f\"Squeezed label shape: {label_np.shape}\")\n",
    "\n",
    "    # Iterate through the batch and save each sample\n",
    "    for i in range(data_np.shape[0]):  # Iterate through the batch\n",
    "        data_filename = os.path.join(save_dir, f'{mode}_data_batch{batch_idx}_instance_{i}.nii.gz')\n",
    "        label_filename = os.path.join(save_dir, f'{mode}_label_batch{batch_idx}_instance_{i}.nii.gz')\n",
    "\n",
    "        nib.save(nib.Nifti1Image(data_np[i], np.eye(4)), data_filename)\n",
    "        nib.save(nib.Nifti1Image(label_np[i], np.eye(4)), label_filename)\n",
    "\n",
    "        print(f\"Saved {data_filename}\")\n",
    "        print(f\"Saved {label_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815174f1-5112-4379-b9db-9dc16fe1b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving DataLoader for training data...\n",
      "Batch 1: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_data_batch0_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_label_batch0_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_data_batch0_instance_1.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_label_batch0_instance_1.nii.gz\n",
      "Batch 2: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_data_batch1_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_label_batch1_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_data_batch1_instance_1.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/train/train_label_batch1_instance_1.nii.gz\n",
      "\n",
      "Saving DataLoader for validation data...\n",
      "Batch 1: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_data_batch0_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_label_batch0_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_data_batch0_instance_1.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_label_batch0_instance_1.nii.gz\n",
      "Batch 2: Data shape: torch.Size([2, 1, 256, 256, 128]), Label shape: torch.Size([2, 1, 256, 256, 128])\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_data_batch1_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_label_batch1_instance_0.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_data_batch1_instance_1.nii.gz\n",
      "Saved /workspace/RibCage/saved_nifti_D_I_D_B/val/val_label_batch1_instance_1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Directory where you want to save the NIfTI files\n",
    "save_dir_train = '/workspace/RibCage/saved_nifti_D_I_D_B/train'\n",
    "save_dir_val = '/workspace/RibCage/saved_nifti_D_I_D_B/val'\n",
    "\n",
    "# Iterate through the DataLoader and save first two batches from training\n",
    "print(\"\\nSaving DataLoader for training data...\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    data_tensor = batch['data']\n",
    "    label_tensor = batch['label']\n",
    "    \n",
    "    print(f'Batch {i + 1}: Data shape: {data_tensor.shape}, Label shape: {label_tensor.shape}')\n",
    "    save_nifti(data_tensor, label_tensor, save_dir_train, i, is_train=True)\n",
    "    \n",
    "    if i == 1:  # Save only the first two batches\n",
    "        break\n",
    "\n",
    "# Iterate through the DataLoader and save first two batches from validation\n",
    "print(\"\\nSaving DataLoader for validation data...\")\n",
    "for i, batch in enumerate(val_loader):\n",
    "    data_tensor = batch['data']\n",
    "    label_tensor = batch['label']\n",
    "    \n",
    "    print(f'Batch {i + 1}: Data shape: {data_tensor.shape}, Label shape: {label_tensor.shape}')\n",
    "    save_nifti(data_tensor, label_tensor, save_dir_val, i, is_train=False)\n",
    "    \n",
    "    if i == 1:  # Save only the first two batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b53b8ca6-1ba2-459f-b07e-9740007b5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calculates the Dice loss between the prediction and the ground truth.\n",
    "    Handles cases where the output or ground truth is all zeros.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays to compare voxel-wise\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "\n",
    "    # Calculate intersection and Dice score\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    union = pred_flat.sum() + target_flat.sum()\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return 1 - dice\n",
    "\n",
    "# Binary Cross Entropy Loss\n",
    "criterion1 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Dice Loss\n",
    "criterion2 = dice_loss\n",
    "\n",
    "# Combined loss function\n",
    "def combined_loss(pred, target):\n",
    "    bce_loss = criterion1(pred, target)\n",
    "    dice_loss_val = criterion2(torch.sigmoid(pred), target)\n",
    "    return bce_loss + dice_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d339ca3-8a77-4b85-92f6-6130f48409e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 2.62 MiB is free. Process 21520 has 23.68 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 892.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#criterion1 = nn.BCEWithLogitsLoss()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#criterion2 = dice_loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m criterion \u001b[38;5;241m=\u001b[39m combined_loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 2.62 MiB is free. Process 21520 has 23.68 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 892.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Model Definition using UNet\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  \n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    encoder_depth=5,\n",
    "    decoder_channels=(256, 128, 64, 32, 16),\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_attention_type=None,\n",
    "    aux_params=None,\n",
    "    strides=((2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2))\n",
    ")\n",
    "\n",
    "# Set up device and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#criterion1 = nn.BCEWithLogitsLoss()\n",
    "#criterion2 = dice_loss\n",
    "criterion = combined_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Optional: Mixed Precision Training with GradScaler\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1dc8e7c-a31a-4979-be90-3e697b129cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Checkpoints\n",
    "def save_checkpoint(state, filename='checkpoint-defected-encoder.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    logger.info(f\"=> Checkpoint saved to '{filename}'\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename='checkpoint-defected-encoder.pth.tar'):\n",
    "    if os.path.isfile(filename):\n",
    "        logger.info(f\"=> Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger.info(f\"=> Loaded checkpoint '{filename}' (epoch {checkpoint['epoch']})\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.info(f\"=> No checkpoint found at '{filename}'\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f684b759-e9ec-4751-92e9-ecd625703868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instance_data(data, outputs, labels, epoch, is_train, instance_idx):\n",
    "    mode = 'train' if is_train else 'val'\n",
    "    output_dir = f'/workspace/RibCage/instances_D_I_D_B/instance_data_epoch_{epoch}/{mode}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert tensors to numpy arrays and cast to float32 (which is supported by NIfTI)\n",
    "    data_np = data.cpu().numpy().astype(np.float32)\n",
    "    outputs_np = outputs.cpu().detach().numpy().astype(np.float32)\n",
    "    labels_np = labels.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    logger.debug(f\"Original data shape: {data_np.shape}\")\n",
    "    logger.debug(f\"Original output shape: {outputs_np.shape}\")\n",
    "    logger.debug(f\"Original labels shape: {labels_np.shape}\")\n",
    "\n",
    "    # Expected shape (D, H, W)\n",
    "    expected_shape = (256, 256, 128)\n",
    "\n",
    "    # If shape has a singleton dimension at index 0, squeeze it out\n",
    "    if data_np.shape[0] == 1:\n",
    "        data_np = data_np.squeeze(0)\n",
    "    if outputs_np.shape[0] == 1:\n",
    "        outputs_np = outputs_np.squeeze(0)\n",
    "    if labels_np.shape[0] == 1:\n",
    "        labels_np = labels_np.squeeze(0)\n",
    "\n",
    "    # Check the shape after squeezing\n",
    "    logger.debug(f\"Squeezed data shape: {data_np.shape}\")\n",
    "    logger.debug(f\"Squeezed output shape: {outputs_np.shape}\")\n",
    "    logger.debug(f\"Squeezed labels shape: {labels_np.shape}\")\n",
    "\n",
    "    # Check the shape before saving (expecting 256x256x128)\n",
    "    if data_np.shape != expected_shape:\n",
    "        raise ValueError(f\"Data shape {data_np.shape} does not match expected shape {expected_shape}\")\n",
    "    if outputs_np.shape != expected_shape:\n",
    "        raise ValueError(f\"Outputs shape {outputs_np.shape} does not match expected shape {expected_shape}\")\n",
    "    if labels_np.shape != expected_shape:\n",
    "        raise ValueError(f\"Labels shape {labels_np.shape} does not match expected shape {expected_shape}\")\n",
    "\n",
    "    # Save input data\n",
    "    nib.save(nib.Nifti1Image(data_np, np.eye(4)), f'{output_dir}/instance_{instance_idx}_input.nii.gz')\n",
    "    \n",
    "    # Save model outputs\n",
    "    nib.save(nib.Nifti1Image(outputs_np, np.eye(4)), f'{output_dir}/instance_{instance_idx}_output.nii.gz')\n",
    "    \n",
    "    # Save ground truth labels\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), f'{output_dir}/instance_{instance_idx}_label.nii.gz')\n",
    "\n",
    "    logger.info(f\"Saved instance data for instance {instance_idx} in epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbbb3489-3205-484d-b855-833dea05c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, dataloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs = batch['data'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Get encoder output (latent representation)\n",
    "            features = model.encoder(inputs)\n",
    "\n",
    "            logger.debug(f\"Val encoder output shape: {len(features)}\")\n",
    "            \n",
    "            # Save latent representation\n",
    "            #save_latent_representation(features, epoch, batch_idx, is_train=False)\n",
    "            \n",
    "            # Continue with the rest of the forward pass\n",
    "            decoder_output = model.decoder(*features)\n",
    "\n",
    "            logger.debug(f\"Val decoder output shape: {decoder_output.shape}\")\n",
    "            \n",
    "            outputs = model.segmentation_head(decoder_output)\n",
    "\n",
    "            logger.debug(f\"Val output shape: {outputs.shape}\")\n",
    "            \n",
    "            logger.debug(f\"Max Val output value: {torch.max(outputs)}, Min Val output value: {torch.min(outputs)}\")\n",
    "            logger.debug(f\"Max Val label value: {torch.max(labels)}, Min Val label value: {torch.min(labels)}\")\n",
    "\n",
    "\n",
    "            binarized_outputs = torch.where(labels == 0, torch.zeros_like(outputs), torch.ones_like(outputs))\n",
    "\n",
    "            binarized_labels = torch.where(labels == 0, torch.zeros_like(labels), torch.ones_like(labels))\n",
    "\n",
    "\n",
    "            logger.debug(f\"Max Val output value: {torch.max(binarized_outputs)}, Min Val output value: {torch.min(binarized_outputs)}\")\n",
    "            logger.debug(f\"Max Val label value: {torch.max(binarized_labels)}, Min Val label value: {torch.min(binarized_labels)}\")\n",
    "\n",
    "            \n",
    "            loss = criterion1(outputs, labels) + criterion2(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Save data for each instance in the validation set\n",
    "            for i in range(inputs.size(0)):\n",
    "                save_instance_data(inputs[i], outputs[i], labels[i], epoch, False, batch_idx * dataloader.batch_size + i)\n",
    "\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1c22d6c-4a2e-407d-a6eb-077d85c5a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_interval=5, early_stopping_patience=5):\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            inputs = batch['data'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logger.debug(f\"Training Epoch {epoch}, Batch {batch_idx} - Input shape: {inputs.shape}, Label shape: {labels.shape}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # Mixed precision\n",
    "                # Get encoder output (latent representation)\n",
    "                features = model.encoder(inputs)\n",
    "\n",
    "                logger.debug(f\"Training Epoch {epoch}, Batch {batch_idx} - Train Encoder output shapes: {[f.shape for f in features]}\")\n",
    "                \n",
    "                # Save latent representation\n",
    "                #save_latent_representation(features, epoch, batch_idx, is_train=True)\n",
    "                \n",
    "                # Continue with the rest of the forward pass\n",
    "                decoder_output = model.decoder(*features)\n",
    "                outputs = model.segmentation_head(decoder_output)\n",
    "\n",
    "                logger.debug(f\"Training Epoch {epoch}, Batch {batch_idx} - Train Output shape: {outputs.shape}\")\n",
    "\n",
    "                logger.debug(f\"Max output value: {torch.max(outputs)}, Min output value: {torch.min(outputs)}\")\n",
    "                logger.debug(f\"Max label value: {torch.max(labels)}, Min label value: {torch.min(labels)}\")\n",
    "\n",
    "                inverted_outputs = torch.where(outputs == 0, torch.ones_like(outputs), torch.zeros_like(outputs))\n",
    "\n",
    "                #logger.debug(f\"Max inverted output value: {torch.max(inverted_outputs)}, Min inverted output value: {torch.min(inverted_outputs)}\")\n",
    "\n",
    "                #missing_outputs= torch.multiply(inverted_outputs,labels)\n",
    "\n",
    "                #inverted_labels = torch.where(labels == 0, torch.ones_like(outputs), torch.zeros_like(outputs))\n",
    "\n",
    "                #extra_outputs = torch.multiply(outputs, inverted_labels)\n",
    "\n",
    "                #zero_labels = torch.zeros_like(labels)\n",
    "\n",
    "                binarized_outputs = torch.where(outputs >= 0, torch.ones_like(outputs), torch.zeros_like(outputs))\n",
    "\n",
    "                binarized_labels = torch.where(labels == 0, torch.zeros_like(labels), torch.ones_like(labels))\n",
    "\n",
    "                logger.debug(f\"Max train binarized_outputs value: {torch.max(binarized_outputs)}, Min train binarized_outputs value: {torch.min(binarized_outputs)}\")\n",
    "                logger.debug(f\"Max train binarized_labels value: {torch.max(binarized_labels)}, Min train binarized_labels value: {torch.min(binarized_labels)}\")\n",
    "                \n",
    "                loss = criterion1(binarized_outputs, binarized_labels) + criterion2(binarized_outputs, binarized_labels)\n",
    "\n",
    "                logger.debug(f\"Loss before scaling: {loss.item()}\")\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Log gradients before and after clipping\n",
    "            #for param in model.parameters():\n",
    "            #    if param.grad is not None:\n",
    "            #        logger.debug(f\"Before clipping - Max gradient: {torch.max(param.grad)}, Min gradient: {torch.min(param.grad)}\")\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            #for param in model.parameters():\n",
    "            #    if param.grad is not None:\n",
    "            #        logger.debug(f\"After clipping - Max gradient: {torch.max(param.grad)}, Min gradient: {torch.min(param.grad)}\")\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Save data for each instance in the training set\n",
    "            #for i in range(inputs.size(0)):\n",
    "            #    save_instance_data(inputs[i], outputs[i], labels[i], epoch, True, batch_idx * train_loader.batch_size + i)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        val_loss = val_model(model, val_loader, criterion, epoch)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        logger.info(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the best model every five intervals\n",
    "        if ((epoch + 1) // checkpoint_interval) % 5 == 0:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                no_improvement_counter = 0\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }\n",
    "                save_checkpoint(state, f'/workspace/RibCage/implants-encoder-models-dice-bce/checkpoint_best_interval_{(epoch + 1) // checkpoint_interval}.pth.tar')\n",
    "            else:\n",
    "                no_improvement_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improvement_counter >= early_stopping_patience:\n",
    "            logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    logger.info('Training and Validation finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb82af47-2511-4a67-9d3d-cd1bda547cb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Started\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_interval, early_stopping_patience)\u001b[0m\n\u001b[1;32m     54\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion1(binarized_outputs, binarized_labels) \u001b[38;5;241m+\u001b[39m criterion2(binarized_outputs, binarized_labels)\n\u001b[1;32m     56\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss before scaling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Log gradients before and after clipping\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#for param in model.parameters():\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#    if param.grad is not None:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#        logger.debug(f\"Before clipping - Max gradient: {torch.max(param.grad)}, Min gradient: {torch.min(param.grad)}\")\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "logger.info('Training Started')\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db208c-25e9-404d-89c3-7b08aa6aead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb5750-4c4f-4ebc-a645-7a2862f5fbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea88698-94b7-4ff0-8b98-59bce9183e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b399f-1a27-4e96-b8a1-8594631cb4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
